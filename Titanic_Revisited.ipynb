{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# use r preceding windows filepath when \\ returns error\n",
    "train = pd.read_csv(r'C:\\Users\\Me\\Kaggle\\Titanic_revisited\\data\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\Me\\Kaggle\\Titanic_revisited\\data\\test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StudentizedOLSClassifier\n",
    "> This is a custom classifier created using Scikit Learn's project template for custom estimators.\n",
    "> It uses statsmodels OLS for the initial fit. Predict then uses the same OLS model to make a \n",
    "> prediction. The median of the predictions is subtracted from the prediction to create an estimated \n",
    "> residual. The estimated residual is then divided by the standard deviation of the residuals from the fit\n",
    "> to create an estimated studentized residual. These studentized residuals are then tested against the\n",
    "> hyperparameter \"threshold\" to determine the label. All  values greater than or equal to the threshold are \n",
    "> labeled True, and values less than the threshold are labeled False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "class StudentizedOLSClassifier(BaseEstimator, ClassifierMixin ):\n",
    "    \n",
    "    def __init__(self, threshold = 'threshold'):\n",
    "        \n",
    "        #decision threshold of studentized residuals\n",
    "        self.threshold = threshold    \n",
    "        np.random.seed(SEED)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y) \n",
    "        \n",
    "        #convert to df\n",
    "        self.X_ = pd.DataFrame(X)\n",
    "        self.y_ = pd.DataFrame(y)         \n",
    "        \n",
    "        #Fit OLS model\n",
    "        self.ols_mod = OLS(endog = self.y_, exog = self.X_)\n",
    "        self.ols_result = self.ols_mod.fit()\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Check if fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "        \n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        X_n = pd.DataFrame(X)\n",
    "        \n",
    "        #OLS prediction       \n",
    "        prediction = self.ols_result.predict(X_n)        \n",
    "        \n",
    "        #calculate outlier and influence measures for OLS result\n",
    "        inf = OLSInfluence(self.ols_result)\n",
    "        \n",
    "        #Staandard Deviation of studentized residuals\n",
    "        std = inf.resid_std\n",
    "        \n",
    "        \"\"\"\n",
    "        Subtract the median of the predictions from the predictions to create an estimated residual.\n",
    "        Then divide the estiamted residual by the by the estimated standard deviation, the\n",
    "        standard deviation of the residuals from training, to create an estimated studentized residual.\n",
    "           \n",
    "        \"\"\" \n",
    "        # estimated residual\n",
    "        estimated_residual = prediction - np.nanmedian(prediction)\n",
    "        \n",
    "        #estiamted studentized residual\n",
    "        stud_res = estimated_residual/np.nanmean(std)    #estimate using mean\n",
    "        #stud_res = prediction/np.nanmedian(std)         #estimate using median\n",
    "        \n",
    "        #create predictions based on the threshold\n",
    "        self.preds = []        \n",
    "        for res in stud_res:\n",
    "            if res >= self.threshold:    \n",
    "                self.preds.append(True)\n",
    "            else:\n",
    "                self.preds.append(False)\n",
    "                \n",
    "        return self.preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved Age Interpolation based on Pclass, Parch, Sibsp\n",
    "\n",
    "index_NaN_age = list(train[\"Age\"][train[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = train[\"Age\"].median()\n",
    "    age_pred = train[\"Age\"][((train['SibSp'] == train.iloc[i][\"SibSp\"]) & (train['Parch'] == train.iloc[i][\"Parch\"]) & (train['Pclass'] == train.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        train['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        train['Age'].iloc[i] = age_med\n",
    "        \n",
    "# Filling missing value of Age in test\n",
    "\n",
    "index_NaN_age = list(test[\"Age\"][test[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = test[\"Age\"].median()\n",
    "    age_pred = test[\"Age\"][((test['SibSp'] == test.iloc[i][\"SibSp\"]) & (test['Parch'] == test.iloc[i][\"Parch\"]) & (test['Pclass'] == test.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        test['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        test['Age'].iloc[i] = age_med\n",
    "\n",
    "#Add title variable\n",
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\n",
    "train[\"Title\"] = pd.Series(dataset_title)\n",
    "train[\"Title\"].head()\n",
    "\n",
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test[\"Name\"]]\n",
    "test[\"Title\"] = pd.Series(dataset_title)\n",
    "test[\"Title\"].head()\n",
    "\n",
    "# Convert to categorical values Title train\n",
    "train[\"Title\"] = train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "train[\"Title\"] = train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "train[\"Title\"] = train[\"Title\"].astype(int)\n",
    "\n",
    "# Convert to categorical values Title test\n",
    "test[\"Title\"] = test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "test[\"Title\"] = test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "test[\"Title\"] = test[\"Title\"].astype(int)\n",
    "\n",
    "# Drop Name variable\n",
    "train.drop(labels = [\"Name\"], axis = 1, inplace = True)\n",
    "test.drop(labels = [\"Name\"], axis = 1, inplace = True)\n",
    "\n",
    "# Create a family size descriptor from SibSp and Parch\n",
    "\n",
    "train[\"Fsize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n",
    "# Create new feature of family size\n",
    "train['Single'] = train['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "train['SmallF'] = train['Fsize'].map(lambda s: 1 if s == 2  else 0)\n",
    "train['MedF']   = train['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "train['LargeF'] = train['Fsize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "\n",
    "test[\"Fsize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n",
    "test['Single'] = test['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "test['SmallF'] = test['Fsize'].map(lambda s: 1 if s == 2  else 0)\n",
    "test['MedF']   = test['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "test['LargeF'] = test['Fsize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "\n",
    "\n",
    "# Create the column Child and assign to 'NaN'\n",
    "train[\"Child\"] = float('NaN')\n",
    "test[\"Child\"] = float('NaN')\n",
    "\n",
    "# Assign 1 to passengers < 20, 0 to those >= 20*******************************\n",
    "age_var = 9\n",
    "train[\"Child\"][train[\"Age\"] < age_var] = 1\n",
    "train[\"Child\"][train[\"Age\"] >= age_var] = 0\n",
    "\n",
    "test[\"Child\"][test[\"Age\"] < age_var] = 1\n",
    "test[\"Child\"][test[\"Age\"] >= age_var] = 0\n",
    "\n",
    "# Convert male and female groups to integer form\n",
    "train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\n",
    "train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n",
    "test[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\n",
    "test[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "# Embarked to int\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n",
    "test[\"Embarked\"][test[\"Embarked\"] == \"S\"] = 0\n",
    "test[\"Embarked\"][test[\"Embarked\"] == \"C\"] = 1\n",
    "test[\"Embarked\"][test[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "test.Fare[152] = test.Fare.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "for f in train.columns: \n",
    "    if train[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "        lbl.fit(list(train[f].values)) \n",
    "        train[f] = lbl.transform(list(train[f].values))\n",
    "        \n",
    "for f in test.columns: \n",
    "    if test[f].dtype=='object': \n",
    "       lbl = preprocessing.LabelEncoder() \n",
    "       lbl.fit(list(test[f].values)) \n",
    "       test[f] = lbl.transform(list(test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "#Scoring Function**********************************************************************************\n",
    "def compute_score(clf, X, y, scoring='accuracy'):\n",
    "    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n",
    "    return np.mean(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\", \"Child\", \n",
    "                    \"Title\", \"Fsize\", \"Single\", \"SmallF\", \"MedF\", \"LargeF\"]]\n",
    "train_features = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\", \"Child\", \n",
    "                    \"Title\", \"Fsize\", \"Single\", \"SmallF\", \"MedF\", \"LargeF\"]].values \n",
    "\n",
    "\n",
    "target = train[\"Survived\"].values    \n",
    "\n",
    "test_features = test[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\", \"Child\",\n",
    "                      \"Title\", \"Fsize\", \"Single\", \"SmallF\", \"MedF\", \"LargeF\"]].values   \n",
    "\n",
    "test_data = test[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\", \"Child\",\n",
    "                      \"Title\", \"Fsize\", \"Single\", \"SmallF\", \"MedF\", \"LargeF\"]]\n",
    "\n",
    "print(train_data.columns.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=target)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                            verbose_eval=True,metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], target,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #print(xgb.cv.results)    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(target, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob))\n",
    "                    \n",
    "#    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = train_data.columns#[x for x in train_data.columns]# if x not in [ target]] #[train_features]#\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=206,\n",
    "                     reg_alpha =0.02,\n",
    "                     reg_lambda =0.0,                    \n",
    "                     max_depth=9,\n",
    "                     min_child_weight=4,\n",
    "                     gamma=0.1,\n",
    "                     subsample=0.6,\n",
    "                     colsample_bytree=0.5,\n",
    "                     objective= 'binary:logistic',\n",
    "                     nthread=3,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=97263)\n",
    "\n",
    "modelfit(xgb1, train_data, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_gs = True\n",
    "if run_gs:\n",
    "    XGB_Param = XGBClassifier()\n",
    "    \n",
    "   \n",
    "    \n",
    "    param_grid_1 = { 'learning_rate' : [0.1], \n",
    "                    'reg_alpha':[0.02],\n",
    "                     'reg_lambda':[0.0],\n",
    "                    'nthread':[3],\n",
    "                    'n_estimators':range(100,2000,50) ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[97263]}\n",
    "    \n",
    "    \n",
    "    \n",
    "    param_grid_1X = { 'learning_rate' : [0.09,0.1,0.11], \n",
    "                    'reg_alpha':[0.01,0.02,0.03],\n",
    "                     'reg_lambda':[0.0,0.1],\n",
    "                    'nthread':[3],\n",
    "                    'n_estimators':[206] ,  #206\n",
    "                    'min_child_weight':[3,4,5],\n",
    "                     'max_depth':[8,9,10],\n",
    "                     'gamma':[0,0.1,0.2],\n",
    "                     'subsample':[0.5,0.6,0.7],\n",
    "                     'colsample_bytree':[0.4,0.5,0.6],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[0.9,1], \n",
    "                      'seed':[97263]}\n",
    "\n",
    "    \n",
    "    param_grid_13 = { 'learning_rate' : [0.1], \n",
    "                    'reg_alpha':[0.02],\n",
    "                     'reg_lambda':[0.0],\n",
    "                    'nthread':[3],\n",
    "                    'n_estimators':[206] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':range(0,100000,321)}\n",
    "\n",
    "\n",
    "    param_grid_12 = { 'learning_rate' : [0.1], \n",
    "                    'reg_alpha':[0.02],\n",
    "                     'reg_lambda':[0.0],\n",
    "                    'nthread':[3],\n",
    "                    'n_estimators':[207] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    \n",
    "    param_grid_11 = { 'reg_alpha':[i/100.0 for i in range(0,5)],\n",
    "                     'reg_lambda':[i/10.0 for i in range(0,5)],\n",
    "                    'nthread':[3],\n",
    "                    'n_estimators':[207] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    \n",
    "    param_grid_10 = { 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "                     'reg_lambda':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "                    'nthread':[3],\n",
    "                    'n_estimators':[207] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    param_grid_9 = { 'nthread':[3],\n",
    "                    'n_estimators':[207] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    param_grid_8 = { 'nthread':[3],\n",
    "                    'n_estimators':[207] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[0.1],\n",
    "                     'subsample':[i/10.0 for i in range(2,9)],\n",
    "                     'colsample_bytree':[i/10.0 for i in range(2,9)],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    \n",
    "    \n",
    "    param_grid_7 = { 'nthread':[3],\n",
    "                    'n_estimators':[208] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[i/20.0 for i in range(1,3)],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    \n",
    "    param_grid_6 = { 'nthread':[3],\n",
    "                    'n_estimators':[208] ,\n",
    "                    'min_child_weight':[4],\n",
    "                     'max_depth':[9],\n",
    "                     'gamma':[i/10.0 for i in range(0,5)],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    \n",
    "    param_grid_5 = { 'nthread':[3],\n",
    "                    'n_estimators':[208] ,\n",
    "                    'min_child_weight':range(0,10,2),\n",
    "                     'max_depth':range(5,13,2),\n",
    "                     'gamma':[0.25],\n",
    "                     'subsample':[0.6],\n",
    "                     'colsample_bytree':[0.5],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    param_grid_4 = { 'nthread':[3],\n",
    "                     'min_child_weight':[2,3,4],\n",
    "                     'max_depth':[9,11,13,15,17],\n",
    "                     'gamma':[0,0.25,0.5],\n",
    "                     'subsample':[0.4,0.5,0.6],\n",
    "                     'colsample_bytree':[0.5, 0.95, 1],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "\n",
    "\n",
    "\n",
    "    param_grid_2 = { 'nthread':[3],\n",
    "                     'min_child_weight':[1,3,5],\n",
    "                     'max_depth':[3,5,7,9],\n",
    "                     'gamma':[0,1],\n",
    "                     'subsample':[0.5,0.8,1],\n",
    "                     'colsample_bytree':[0.5, 0.95, 1],\n",
    "                     'objective': ['binary:logistic'],                       \n",
    "                     'scale_pos_weight':[0,1], \n",
    "                      'seed':[5432]}\n",
    "    \n",
    "    param_grid_3 = { 'learning_rate' : [0.01], \n",
    "                      'n_estimators':[400,600,800,1000],\n",
    "                       'max_depth':(4,15),\n",
    "                        'min_child_weight':[5],\n",
    "                       'gamma':[0],\n",
    "                       'subsample':[0.8],\n",
    "                       'colsample_bytree':[0.95],\n",
    "                       'reg_alpha':[1e-5],\n",
    "                       'objective': ['binary:logistic'], \n",
    "                      'nthread':[4], \n",
    "                      'scale_pos_weight':[1], \n",
    "                      'seed':[5432]}\n",
    "\n",
    "\n",
    "    XGB_model_1 = GridSearchCV(XGB_Param, param_grid=param_grid_1, cv=kfold, scoring=\"roc_auc\", n_jobs=-3, verbose=1)\n",
    "    XGB_model_1.fit(train_data, target)\n",
    "    print(XGB_model_1.best_score_)\n",
    "    print(XGB_model_1.best_params_)\n",
    "\n",
    "else: \n",
    "    param_grid_1 =  {  'learning_rate' : 0.1, \n",
    "                       'reg_alpha':0.02,\n",
    "                      'reg_lambda':0.0,\n",
    "                      'n_estimators':5000, #205\n",
    "                      'max_depth':9,\n",
    "                      'min_child_weight':4, \n",
    "                      'gamma':0.1, \n",
    "                      'subsample':0.6, \n",
    "                      'colsample_bytree':0.5,\n",
    "                      'objective': 'binary:logistic', \n",
    "                      'nthread':3, \n",
    "                      'scale_pos_weight':1, \n",
    "                      'seed':97263}\n",
    "    \n",
    "   \n",
    "    XGB_model_1 = XGBClassifier(**param_grid_1)\n",
    "    XGB_model_1.fit(train_data, target)\n",
    "\n",
    "cv_score = compute_score(XGB_model_1, train_data, target, scoring='accuracy')\n",
    "print(\"cross_val_score=\", cv_score)\n",
    "cv_score = compute_score(XGB_model_1, train_data, target, scoring='roc_auc')\n",
    "print(\"cross_val_score roc auc=\", cv_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "final_pickle = False\n",
    "if final_pickle:\n",
    "    import pickle\n",
    "    pickle_name = 'XGB_1_Pickle.sav'\n",
    "    XGB_pickle = open(pickle_name, 'rb')\n",
    "    XGB_pickled_model = pickle.load(XGB_pickle)\n",
    "    print(\"pickled model\",XGB_pickled_model)\n",
    "    \n",
    "    XGB_cv_score = compute_score(XGB_pickled_model, train_data, target, scoring='accuracy')\n",
    "    print(\"Final pickle_score=\", XGB_cv_score)\n",
    "    final_submit = XGB_pickled_model.predict(test_data)\n",
    "\n",
    "else:\n",
    "    final_submit = XGB_model_1.predict(test_data)\n",
    "    \n",
    "    XGB_cv_score = compute_score(XGB_model_1, train_data, target, scoring='accuracy')\n",
    "    print(\"Final cv_score=\", XGB_cv_score)\n",
    "    cv_score = compute_score(XGB_model_1, train_data, target, scoring='roc_auc')\n",
    "    print(\"cross_val_score roc auc=\", cv_score)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "#Final array*************************************************************************************\n",
    "PassengerId =np.array(test[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(final_submit, PassengerId, columns = [\"Survived\"])\n",
    "\n",
    "my_solution.to_csv(\"solution_XGB_3.csv\", index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sound******************************\n",
    "import winsound\n",
    "duration = 500  # millisecond\n",
    "freq = 500\n",
    "freq_2 = 450  # Hz\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq_2, duration)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq_2, duration)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq_2, duration)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq_2, 1000)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(freq_2, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
